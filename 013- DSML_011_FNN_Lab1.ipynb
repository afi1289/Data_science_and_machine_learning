{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of some data\n",
    "np.random.seed (245)\n",
    "nobs =10000\n",
    "x1 = 2 * np.random.rand(nobs , 1)\n",
    "x2 = np.random.uniform(size=nobs).reshape((nobs ,1))\n",
    "y = -0.5 + 2 * x1 - 3*x2 + np.random.normal(loc=0, scale=0.01, size=nobs).reshape((nobs ,1))\n",
    "X = np.c_[np.ones((nobs ,1)),x1,x2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 1.035e+08\n",
      "Date:                Thu, 06 Feb 2020   Prob (F-statistic):               0.00\n",
      "Time:                        12:51:09   Log-Likelihood:                 31811.\n",
      "No. Observations:               10000   AIC:                        -6.362e+04\n",
      "Df Residuals:                    9997   BIC:                        -6.359e+04\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.5003      0.000  -1882.361      0.000      -0.501      -0.500\n",
      "x1             2.0000      0.000   1.15e+04      0.000       2.000       2.000\n",
      "x2            -2.9996      0.000  -8585.142      0.000      -3.000      -2.999\n",
      "==============================================================================\n",
      "Omnibus:                        2.625   Durbin-Watson:                   1.987\n",
      "Prob(Omnibus):                  0.269   Jarque-Bera (JB):                2.647\n",
      "Skew:                          -0.020   Prob(JB):                        0.266\n",
      "Kurtosis:                       3.069   Cond. No.                         6.28\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "OLS_model=sm.OLS(y,X).fit()\n",
    "y_pred_OLS=OLS_model.predict(X)\n",
    "print(OLS_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Neural Network\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed (245)\n",
    "#Learningrate\n",
    "LR=0.05\n",
    "#Initialize the ANN\n",
    "model_ANN= Sequential()\n",
    "#Output Layer\n",
    "model_ANN.add(Dense(1, activation=\"linear\", input_shape=(3,),use_bias=False))\n",
    "#The Optimizer\n",
    "Optimizer= Adam(lr=LR)\n",
    "#Compiling the model\n",
    "model_ANN.compile(optimizer=Optimizer , loss='mean_squared_error')\n",
    "model_ANN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 4s 370us/step - loss: 0.9023\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0844\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0085\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 4.2744e-04\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0356e-04\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0131e-04\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0191e-04\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0138e-04\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0121e-04\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0173e-04\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0146e-04\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0241e-04\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0254e-04\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0236e-04\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0198e-04\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0176e-04\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0178e-04\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0214e-04\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0166e-04\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0197e-04\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0170e-04\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0237e-04\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0268e-04\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0155e-04\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0207e-04\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0188e-04\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 1.0181e-04\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 1.0213e-04\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 1.0360e-04\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 1.0181e-04\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0208e-04\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0249e-04\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0231e-04\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0660e-04\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0313e-04\n",
      "Epoch 36/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0298e-04\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0335e-04\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0288e-04\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0360e-04\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0483e-04\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0352e-04\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0285e-04\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0337e-04\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0440e-04\n",
      "Epoch 45/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0239e-04\n",
      "Epoch 46/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0247e-04\n",
      "Epoch 47/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0391e-04\n",
      "Epoch 48/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0669e-04\n",
      "Epoch 49/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0676e-04\n",
      "Epoch 50/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0565e-04\n",
      "Epoch 51/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0367e-04\n",
      "Epoch 52/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0813e-04\n",
      "Epoch 53/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0472e-04\n",
      "Epoch 54/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0343e-04\n",
      "Epoch 55/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0209e-04\n",
      "Epoch 56/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0328e-04\n",
      "Epoch 57/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.1388e-04\n",
      "Epoch 58/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0475e-04\n",
      "Epoch 59/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0307e-04\n",
      "Epoch 60/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0518e-04\n",
      "Epoch 61/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0291e-04\n",
      "Epoch 62/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.2242e-04\n",
      "Epoch 63/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1269e-04\n",
      "Epoch 64/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0525e-04\n",
      "Epoch 65/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0396e-04\n",
      "Epoch 66/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0311e-04\n",
      "Epoch 67/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0465e-04\n",
      "Epoch 68/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0374e-04\n",
      "Epoch 69/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0615e-04\n",
      "Epoch 70/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0960e-04\n",
      "Epoch 71/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0294e-04\n",
      "Epoch 72/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0223e-04\n",
      "Epoch 73/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0685e-04\n",
      "Epoch 74/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0890e-04\n",
      "Epoch 75/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1355e-04\n",
      "Epoch 76/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0568e-04\n",
      "Epoch 77/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2430e-04\n",
      "Epoch 78/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0542e-04\n",
      "Epoch 79/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0738e-04\n",
      "Epoch 80/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1003e-04\n",
      "Epoch 81/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0939e-04\n",
      "Epoch 82/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0431e-04\n",
      "Epoch 83/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1136e-04\n",
      "Epoch 84/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0627e-04\n",
      "Epoch 85/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1136e-04\n",
      "Epoch 86/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0390e-04\n",
      "Epoch 87/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1511e-04\n",
      "Epoch 88/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1010e-04\n",
      "Epoch 89/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1237e-04\n",
      "Epoch 90/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1100e-04\n",
      "Epoch 91/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.0620e-04\n",
      "Epoch 92/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1230e-04\n",
      "Epoch 93/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.0583e-04\n",
      "Epoch 94/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.0470e-04\n",
      "Epoch 95/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.0708e-04\n",
      "Epoch 96/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1467e-04\n",
      "Epoch 97/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.0501e-04\n",
      "Epoch 98/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1670e-04\n",
      "Epoch 99/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.2230e-04\n",
      "Epoch 100/200\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.1721e-0 - 0s 9us/step - loss: 1.1785e-04\n",
      "Epoch 101/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1750e-04\n",
      "Epoch 102/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1175e-04\n",
      "Epoch 103/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.0990e-04\n",
      "Epoch 104/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1014e-04\n",
      "Epoch 105/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.2999e-04\n",
      "Epoch 106/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1228e-04\n",
      "Epoch 107/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1484e-04\n",
      "Epoch 108/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1159e-04\n",
      "Epoch 109/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1305e-04\n",
      "Epoch 110/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.0560e-04\n",
      "Epoch 111/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1014e-04\n",
      "Epoch 112/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1027e-04\n",
      "Epoch 113/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.2857e-04\n",
      "Epoch 114/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.1012e-04\n",
      "Epoch 115/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1632e-04\n",
      "Epoch 116/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2359e-04\n",
      "Epoch 117/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.0873e-04\n",
      "Epoch 118/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.0757e-04\n",
      "Epoch 119/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2140e-04\n",
      "Epoch 120/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0685e-04\n",
      "Epoch 121/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.2949e-04\n",
      "Epoch 122/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.3625e-04\n",
      "Epoch 123/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1634e-04\n",
      "Epoch 124/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.3131e-04\n",
      "Epoch 125/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.3465e-04\n",
      "Epoch 126/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.2331e-04\n",
      "Epoch 127/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.5444e-04\n",
      "Epoch 128/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0834e-04\n",
      "Epoch 129/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.2356e-04\n",
      "Epoch 130/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1326e-04\n",
      "Epoch 131/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1175e-04\n",
      "Epoch 132/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2429e-04\n",
      "Epoch 133/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1554e-04\n",
      "Epoch 134/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1802e-04\n",
      "Epoch 135/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1728e-04\n",
      "Epoch 136/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2061e-04\n",
      "Epoch 137/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.3783e-04\n",
      "Epoch 138/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1630e-04\n",
      "Epoch 139/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1303e-04\n",
      "Epoch 140/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2434e-04\n",
      "Epoch 141/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2017e-04\n",
      "Epoch 142/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.1530e-04\n",
      "Epoch 143/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.0895e-04\n",
      "Epoch 144/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.3717e-04\n",
      "Epoch 145/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2454e-04\n",
      "Epoch 146/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.3215e-04\n",
      "Epoch 147/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1112e-04\n",
      "Epoch 148/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.4731e-04\n",
      "Epoch 149/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.8840e-04\n",
      "Epoch 150/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0721e-04\n",
      "Epoch 151/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.4593e-04\n",
      "Epoch 152/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1475e-04\n",
      "Epoch 153/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1603e-04\n",
      "Epoch 154/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2431e-04\n",
      "Epoch 155/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1305e-04\n",
      "Epoch 156/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1013e-04\n",
      "Epoch 157/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.3673e-04\n",
      "Epoch 158/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.6499e-04\n",
      "Epoch 159/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.4489e-04\n",
      "Epoch 160/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.2570e-04\n",
      "Epoch 161/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1118e-04\n",
      "Epoch 162/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1501e-04\n",
      "Epoch 163/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0940e-04\n",
      "Epoch 164/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0614e-04\n",
      "Epoch 165/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0658e-04\n",
      "Epoch 166/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1907e-04\n",
      "Epoch 167/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1595e-04\n",
      "Epoch 168/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.3284e-04\n",
      "Epoch 169/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1559e-04\n",
      "Epoch 170/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.0577e-04\n",
      "Epoch 171/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1361e-04\n",
      "Epoch 172/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1976e-04\n",
      "Epoch 173/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1416e-04\n",
      "Epoch 174/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1926e-04\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2295e-04\n",
      "Epoch 176/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.3110e-04\n",
      "Epoch 177/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1122e-04\n",
      "Epoch 178/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.3940e-04\n",
      "Epoch 179/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.3020e-04\n",
      "Epoch 180/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.3307e-04\n",
      "Epoch 181/200\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 1.4774e-04\n",
      "Epoch 182/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.7670e-04\n",
      "Epoch 183/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.4396e-04\n",
      "Epoch 184/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.4236e-04\n",
      "Epoch 185/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1324e-04\n",
      "Epoch 186/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.3034e-04\n",
      "Epoch 187/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.2637e-04\n",
      "Epoch 188/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1499e-04\n",
      "Epoch 189/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.8547e-04\n",
      "Epoch 190/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1025e-04\n",
      "Epoch 191/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.1156e-04\n",
      "Epoch 192/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2409e-04\n",
      "Epoch 193/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2656e-04\n",
      "Epoch 194/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1648e-04\n",
      "Epoch 195/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.1662e-04\n",
      "Epoch 196/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.3381e-04\n",
      "Epoch 197/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.2202e-04\n",
      "Epoch 198/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.3038e-04\n",
      "Epoch 199/200\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 1.3321e-04\n",
      "Epoch 200/200\n",
      "10000/10000 [==============================] - 0s 8us/step - loss: 1.2919e-04\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "history_ANN=model_ANN.fit(\n",
    "X, # training data\n",
    "y, # training targets\n",
    "epochs=200,verbose=1,batch_size =256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const [-0.49678433]\n",
      "Beta1 [2.0022402]\n",
      "Beta1 [-2.9966059]\n",
      "MSE ANN: 0.0001550536205914129\n",
      "MSE OLS: 0.00010103468380802305\n"
     ]
    }
   ],
   "source": [
    "#Estimated Weights\n",
    "print('const',model_ANN.layers [0]. get_weights ()[0][0])\n",
    "print('Beta1',model_ANN.layers [0]. get_weights ()[0][1])\n",
    "print('Beta1',model_ANN.layers [0]. get_weights ()[0][2])\n",
    "#Evaluate the fit\n",
    "y_pred_ANN=model_ANN.predict(X)\n",
    "print(\"MSE ANN:\", mean_squared_error(np.ravel(y), np.ravel(y_pred_ANN )))\n",
    "print(\"MSE OLS:\", mean_squared_error(np.ravel(y), np.ravel(y_pred_OLS )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(245)\n",
    "nobs=10000\n",
    "mean = np.zeros(2)\n",
    "cov = np.eye(2)\n",
    "X=np.random.multivariate_normal(mean,cov,nobs)\n",
    "X=sm.add_constant(X)\n",
    "\n",
    "y_star= X.T[0]+ 5*X.T[1]+ 4*X.T[2] + np.random.logistic(size=nobs)\n",
    "\n",
    "y=np.where(y_star>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.191293\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Thu, 06 Feb 2020   Pseudo R-squ.:                  0.7216\n",
      "Time:                        13:01:21   Log-Likelihood:                -1912.9\n",
      "converged:                       True   LL-Null:                       -6872.2\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.9896      0.047     21.074      0.000       0.898       1.082\n",
      "x1             5.1413      0.129     39.769      0.000       4.888       5.395\n",
      "x2             4.2387      0.108     39.316      0.000       4.027       4.450\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.17 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "Logit_fit=sm.Logit(y, X).fit()\n",
    "print(Logit_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Neural Network\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed (245)\n",
    "#Learningrate\n",
    "LR=0.05\n",
    "#Initialize the ANN\n",
    "model_ANN= Sequential()\n",
    "#Output Layer\n",
    "model_ANN.add(Dense(1, activation=\"sigmoid\", input_shape=(3,),use_bias=False))\n",
    "#The Optimizer\n",
    "Optimizer= Adam(lr=LR)\n",
    "#Compiling the model\n",
    "model_ANN.compile(optimizer=Optimizer , loss='mean_squared_error')\n",
    "model_ANN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 1s - loss: 0.1223\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.0712\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.0660\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.0637\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.0625\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.0617\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.0613\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.0609\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.0607\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.0605\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.0604\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.0604\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.0604\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.0604\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.0605\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.0607\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.0604\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.0605\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.0605\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.0604\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.0601\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.0600\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.0603\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.0602\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "history_ANN=model_ANN.fit(\n",
    "X, # training data\n",
    "y, # training targets\n",
    "epochs=200,verbose=2,batch_size =256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = Logit_fit.predict(X)\n",
    "y_pred_Logit = (y_score> 0.5). astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const [0.89054304]\n",
      "Beta1 [5.019573]\n",
      "Beta1 [4.227439]\n",
      "Accuracy ANN: 0.9135\n",
      "Accuracy Logit: 0.9153\n"
     ]
    }
   ],
   "source": [
    "#Estimated Weights\n",
    "print('const',model_ANN.layers [0]. get_weights ()[0][0])\n",
    "print('Beta1',model_ANN.layers [0]. get_weights ()[0][1])\n",
    "print('Beta1',model_ANN.layers [0]. get_weights ()[0][2])\n",
    "#Evaluate the fit\n",
    "y_pred_ANN=model_ANN.predict(X)\n",
    "print(\"Accuracy ANN:\", accuracy_score(np.ravel(y), np.ravel(model_ANN.predict_classes(X))))\n",
    "print(\"Accuracy Logit:\", accuracy_score(np.ravel(y), np.ravel(y_pred_Logit )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall ANN: 0.9181096681096681\n",
      "Recall Logit: 0.9256854256854257\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall ANN:\", recall_score(np.ravel(y), np.ravel(model_ANN.predict_classes(X))))\n",
    "print(\"Recall Logit:\", recall_score(np.ravel(y), np.ravel(y_pred_Logit )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "302.86px",
    "left": "auto",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
